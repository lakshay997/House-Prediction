{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House Price Prediction - Advanced regression Assignment \n",
    "### ------------------------------------------------------------------------------------------------------ \n",
    "\n",
    "The solution is divided into the following sections: \n",
    "- Data understanding and exploration\n",
    "- Data cleaning\n",
    "- Data preparation\n",
    "- Model building and evaluation\n",
    "- Observation and inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the important\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataset\n",
    "data = pd.read_csv(\"train.csv\", encoding = 'utf-8')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for column details\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the description of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking duplicates\n",
    "sum(data.duplicated(subset = 'Id')) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for percentage nulls\n",
    "round(100*(data.isnull().sum()/len(data.index)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for outlier in the numerical columns\n",
    "data.describe(percentiles=[.25,.5,.75,.90,.95,.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(x,y):\n",
    "    q1 = x[y].quantile(0.25)\n",
    "    q3 = x[y].quantile(0.75)\n",
    "    value = q3-q1\n",
    "    lower_value  = q1-1.5*value\n",
    "    higer_value = q3+1.5*value\n",
    "    out= x[(x[y]<higer_value) & (x[y]>lower_value)]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the shape of the dataframe\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since, it is clear that there are multiple columns with high nulls, lets group them together\n",
    "data.columns[data.isnull().any()] \n",
    "\n",
    "null = data.isnull().sum()/len(data)*100\n",
    "null = null[null>0]\n",
    "null.sort_values(inplace=True, ascending=False)\n",
    "null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## According to the data dictionary provided, the nulls in these columns indicates the absence of facility which may affect the price\n",
    "## Hence, we will first impute the categorical variables with 'None'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the data dictionary provided, the nulls in these columns indicates the absence of facility which may affect the price\n",
    "# Hence, we will first impute the categorical variables with 'None'\n",
    "null_with_meaning = [\"Alley\", \"MasVnrType\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\n",
    "for i in null_with_meaning:\n",
    "    data[i].fillna(\"none\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nulls once again\n",
    "\n",
    "data.columns[data.isnull().any()] \n",
    "\n",
    "null_2 = data.isnull().sum()/len(data)*100\n",
    "null_2 = null_2[null_2>0]\n",
    "null_2.sort_values(inplace=True, ascending=False)\n",
    "null_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will check these columns one by one\n",
    "data['LotFrontage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['GarageYrBlt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MasVnrArea'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Electrical'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As per the data dictionary \"LotFrontage\" is Linear feet of street connected to property.  \n",
    "# Since it is a numeric with a fair distribution, it can be imputed with similar 'Neighborhood' values\n",
    "\n",
    "data['LotFrontage'] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "data[\"GarageYrBlt\"].fillna(data[\"GarageYrBlt\"].median(), inplace=True)\n",
    "data[\"MasVnrArea\"].fillna(data[\"MasVnrArea\"].median(), inplace=True)\n",
    "data[\"Electrical\"].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosscheck the updated 'LotFrontage' column\n",
    "data['LotFrontage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['GarageYrBlt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MasVnrArea'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Electrical'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the no. of rows retained\n",
    "len(data.index)\n",
    "len(data.index)/1460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some EDA on the cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All numeric (float and int) variables in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numeric = data.select_dtypes(include=['float64', 'int64'])\n",
    "data_numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping ID Column\n",
    "data_numeric = data_numeric.drop(['Id'], axis=1)\n",
    "data_numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 'Sale Price' wrt 'Neighborhood'\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(x=\"Neighborhood\", y=\"SalePrice\", data= data)\n",
    "plt.title(\"Sales Price wrt Neighbourhood\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties in some of the Neighborhoods are high priced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 'overall condition' with respect to 'Saleprice'\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(x=\"OverallCond\", y=\"SalePrice\", data= data)\n",
    "plt.title(\"Sales Price wrt Overall Condition\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 'overall quality' wrt 'Saleprice'\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(x=\"OverallQual\", y=\"SalePrice\", data= data)\n",
    "plt.title(\"Sales Price wrt Overall Quality\")\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase in the overall quality has a direct positive effect on the sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = data.copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the Saleprice figures are skewed towards left, we will apply the log transformation to obtain a centralized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transformation\n",
    "data['SalePrice']=np.log1p(data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "cor = data_numeric.corr()\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlations on a heatmap\n",
    "\n",
    "# figure size\n",
    "plt.figure(figsize=(30,20))\n",
    "\n",
    "# heatmap\n",
    "sns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that some of the variables are correlated\n",
    "\n",
    "#### Before dropping these columns, we will first check their predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the same with a pairplot \n",
    "sns.set()\n",
    "cols = ['SalePrice', 'GrLivArea', 'GarageCars', 'BsmtUnfSF', 'BsmtFinSF1', 'GarageArea', 'TotalBsmtSF', 'YearBuilt', 'TotRmsAbvGrd', 'GarageYrBlt']\n",
    "sns.pairplot(data[cols], size = 2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are correlated and not contributing to 'SalePrice'\n",
    "data = data.drop(['GarageCars'], axis = 1)\n",
    "data = data.drop(['BsmtUnfSF'], axis = 1)\n",
    "data = data.drop(['TotRmsAbvGrd'], axis = 1)\n",
    "data = data.drop(['GarageYrBlt'], axis = 1)    \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numeric columns\n",
    "data.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse some important numeric columns\n",
    "sns.jointplot(x='GrLivArea', y='SalePrice', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing some outliers on lower right side of 'GrLivArea'\n",
    "data = remove_outliers(data,'GrLivArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the dataset is small it isn't advisable to do remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again plotting GeLivArea vs SalePrice\n",
    "sns.jointplot(x = data['GrLivArea'], y = data['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot frontage vs SalePrice \n",
    "sns.jointplot(x = data['LotFrontage'], y = data['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LotArea vs SalePrice\n",
    "sns.jointplot(x = data['LotArea'], y = data['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1stFlrSF vs SalePrice\n",
    "sns.jointplot(x = data['1stFlrSF'], y = data['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ndFlrSF vs SalePrice\n",
    "sns.jointplot(x = data['2ndFlrSF'], y = data['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OverallQual vs SalePrice\n",
    "sns.jointplot(x = data['OverallQual'], y = data['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OverallCond vs SalePrice\n",
    "sns.jointplot(x=data['OverallCond'], y = data['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground or First level houses i.e. '0' second floor Sq.Ft has also a steady increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can derive a column for 'Age of the property' when it was sold: Name it as 'PropAge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PropAge -  Property Age from yearsold - yearbuilt\n",
    "data['PropAge'] = (data['YrSold'] - data['YearBuilt'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PropAge vs SalePrice\n",
    "sns.jointplot(x = data['PropAge'], y = data['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase in Property Age shows a decreasing saleprice trend i.e newer the property, high is the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Now we can drop the column Month sold and Year Sold, Year built and Year remodelled since it will not be required further "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['MoSold'], axis = 1)\n",
    "data = data.drop(['YrSold'], axis = 1)\n",
    "data = data.drop(['YearBuilt'], axis = 1)\n",
    "data = data.drop(['YearRemodAdd'], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Street.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Utilities.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also drop columns that show very low variance and thus not required for predictions\n",
    "data = data.drop(['Street'], axis = 1)\n",
    "data = data.drop(['Utilities'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just to check the variance of these columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# l1 = ['Condition2', 'Heating', 'PoolQC', 'RoofMatl', 'BsmtCond', 'GarageQual', 'GarageCond', 'MiscVal', '3SsnPorch', 'FireplaceQu', 'BsmtHalfBath', 'BsmtFinSF2', 'Alley', 'MiscFeature', 'Fence', 'Functional']\n",
    "l2= data.select_dtypes(include=['float64', 'int64'])\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in l2:\n",
    "    print(data[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data = data.drop(['PoolQC','MiscVal', 'Alley', 'RoofMatl', 'Condition2', 'Heating', 'GarageCond', 'Fence', 'Functional' ], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These Columns were having high null values, some of which were imputed. After imputing, it was found that there was very little variance in the data. So we have decided to drop these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation \n",
    "\n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "Let's now prepare the data and build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Id' from Dataframe\n",
    "\n",
    "data = data.drop(['Id'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type of each feature in data: int, float, object\n",
    "types = data.dtypes\n",
    "#numerical values are either type int or float\n",
    "numeric_type = types[(types == 'int64') | (types == float)] \n",
    "#categorical values are type object\n",
    "categorical_type = types[types == object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(types).reset_index().set_index(0).reset_index()[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we should convert numeric_type to a list to make it easier to work with\n",
    "numerical_columns = list(numeric_type.index)\n",
    "print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical columns\n",
    "categorical_columns = list(categorical_type.index)\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dummy columns to convert categorical into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, drop_first=True )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['SalePrice'], axis=1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting response variable to y\n",
    "y = data['SalePrice']\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PropAge']] = scaler.fit_transform(X_train[['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PropAge']])\n",
    "\n",
    "X_test[['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PropAge']] = scaler.fit_transform(X_test[['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PropAge']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lasso and Ridge Regression\n",
    "\n",
    "Let's now try predicting house prices and perform lasso and ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimension of X_train & y_train\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Lasso\n",
    "\n",
    "# list of alphas to tune\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n",
    " 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n",
    " 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "lasso = Lasso()\n",
    "\n",
    "# cross validation\n",
    "folds = 5\n",
    "model_cv = GridSearchCV(estimator = lasso, \n",
    "                        param_grid = params, \n",
    "                        scoring= 'neg_mean_absolute_error', \n",
    "                        cv = folds, \n",
    "                        return_train_score=True,\n",
    "                        verbose = 1)            \n",
    "\n",
    "model_cv.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results = cv_results[cv_results['param_alpha']<=1]\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting mean test and train scoes with alpha \n",
    "cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Negative Mean Absolute Error')\n",
    "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above graph we can see that the Negative Mean Absolute Error is quite low at alpha = 0.4 and stabilises thereafter, \n",
    "#### but we will choose a low value of alpha to balance the trade-off between Bias-Variance \n",
    "#### and to get the coefficients of smallest of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At alpha = 0.01, even the smallest of negative coefficients that have some predictive power towards 'SalePrice' have been generated\n",
    "\n",
    "alpha = 0.01\n",
    "lasso = Lasso(alpha=alpha)\n",
    "\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The advantage of this technique is clearly visible here as Lasso brings the coefficients of insignificant features to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso model parameters\n",
    "model_parameters = list(lasso.coef_ )\n",
    "model_parameters.insert(0, lasso.intercept_)\n",
    "model_parameters = [round(x, 3) for x in model_parameters]\n",
    "cols = X.columns\n",
    "cols = cols.insert(0, \"constant\")\n",
    "list(zip(cols, model_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso regression\n",
    "lm = Lasso(alpha=0.01)\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# prediction on the test set(Using R2)\n",
    "y_train_pred = lm.predict(X_train)\n",
    "print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\n",
    "y_test_pred = lm.predict(X_test)\n",
    "print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE :', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The R2 values for Train and Test matches well, indicating an optimum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for the coefficients obtained from Lasso\n",
    "mod = list(zip(cols, model_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = pd.DataFrame(mod)\n",
    "para.columns = ['Variable', 'Coeff']\n",
    "para.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the coefficients in ascending order\n",
    "para = para.sort_values((['Coeff']), axis = 0, ascending = False)\n",
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose variables whose coefficients are non-zero\n",
    "pred = pd.DataFrame(para[(para['Coeff'] != 0)])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 16 variables obtained from Lasso Regression can be concluded to have the strong effect on the SalePrice\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lassso_var = list(pred['Variable'])\n",
    "print(Lassso_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lasso = X_train[['GrLivArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF', 'GarageArea', 'BsmtFinSF1', 'Fireplaces', 'LotArea', 'LotFrontage', 'BsmtFullBath', 'Foundation_PConc', 'OpenPorchSF', 'FullBath', 'ScreenPorch', 'WoodDeckSF']]\n",
    "                        \n",
    "X_train_lasso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lasso.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lasso = X_test[['GrLivArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF', 'GarageArea', 'BsmtFinSF1', 'Fireplaces', 'LotArea', 'LotFrontage', 'BsmtFullBath', 'Foundation_PConc', 'OpenPorchSF', 'FullBath', 'ScreenPorch', 'WoodDeckSF']]\n",
    "                        \n",
    "X_test_lasso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of alphas to tune\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n",
    " 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n",
    " 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "# cross validation\n",
    "folds = 5\n",
    "model_cv = GridSearchCV(estimator = ridge, \n",
    "                        param_grid = params, \n",
    "                        scoring= 'neg_mean_absolute_error', \n",
    "                        cv = folds, \n",
    "                        return_train_score=True,\n",
    "                        verbose = 1)            \n",
    "model_cv.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results = cv_results[cv_results['param_alpha']<=5]\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting mean test and train scoes with alpha \n",
    "cv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Negative Mean Absolute Error')\n",
    "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since the Negative Mean Absolute Error stabilises at alpha = 2, we will choose this for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2\n",
    "ridge = Ridge(alpha=alpha)\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge model parameters\n",
    "model_parameters = list(ridge.coef_)\n",
    "model_parameters.insert(0, ridge.intercept_)\n",
    "model_parameters = [round(x, 3) for x in model_parameters]\n",
    "cols = X.columns\n",
    "cols = cols.insert(0, \"constant\")\n",
    "list(zip(cols, model_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression\n",
    "lm = Ridge(alpha=2)\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_train_pred = lm.predict(X_train)\n",
    "print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\n",
    "y_test_pred = lm.predict(X_test)\n",
    "print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE :', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is visible that the model performance is better than Lasso.  The train and the test scores are matching well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for Ridge Coefficients\n",
    "mod_ridge = list(zip(cols, model_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraRFE = pd.DataFrame(mod_ridge)\n",
    "paraRFE.columns = ['Variable', 'Coeff']\n",
    "res=paraRFE.sort_values(by=['Coeff'], ascending = False)\n",
    "res.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the coefficients in ascending order\n",
    "paraRFE = paraRFE.sort_values((['Coeff']), axis = 0, ascending = False)\n",
    "paraRFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## since there were few coefficients at 0, we removed them from features\n",
    "predRFE = pd.DataFrame(paraRFE[(paraRFE['Coeff'] != 0)])\n",
    "predRFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predRFE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "#### Though the model performance by Ridge Regression was better in terms of R2 values of Train and Test, \n",
    "#### it is better to use Lasso, since it brings and assigns a zero value to insignificant features, enabling us to choose\n",
    "#### the predictive variables.\n",
    "#### It is always advisable to use simple yet robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equation can be formulated using the features and coefficients obtained by Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assign the Features as x1, x2.....\n",
    "\n",
    "pred.set_index(pd.Index(['C','x1', 'x2', 'x3', 'x4', 'x5' , 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16']), inplace = True) \n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the final features that should be selected for predicting the price of house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hence the equation:\n",
    "    \n",
    "####    Log(Y) = C + 0.125(x1) + 0.112(x2) +  0.050(x3) + 0.042(x4) + 0.035(x5) + 0.034(x6) + 0.024(x7) +  0.015(x8) + 0.014(x9) + 0.010(x10)\n",
    "####                + 0.010(x11) + 0.005(x12) - 0.007(x13) - 0.007(x14) - 0.008(x15) - 0.095(x16) + Error term(RSS + alpha * (sum of absolute value of coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCE\n",
    "\n",
    "\n",
    "#### Suggestions for Surprise Housing is to keep a check on these predictors affecting the price of the house.\n",
    "\n",
    "#### The higher values of positive coeeficients suggest a high sale value.\n",
    "\n",
    "#### Some of those features are:-\n",
    " |  Feature  |  Description  |\n",
    " |  ---  |  ---  |\n",
    " |  GrLivArea  |  Above grade (ground) living area square feet  |\n",
    " |  OverallQual  |  Rates the overall material and finish of the house  |\n",
    " |  OverallCond  |  Rates the overall condition of the house  |\n",
    " |  TotalBsmtSF  |  Total square feet of basement area  |\n",
    " |  GarageArea   |Size of garage in square feet  |\n",
    "        \n",
    "#### The higher values of negative coeeficients suggest a decrease in sale value.\n",
    "\n",
    "#### Some of those features are:-\n",
    "   |  Feature  |  Description  |\n",
    "   |  ---  |  ---  |\n",
    "   |  PropAge  |  Age of the property at the time of seeling  |\n",
    "   |  MSSubClass  |  Identifies the type of dwelling involved in the sale  |\n",
    "    \n",
    "\n",
    "#### When the market value of the property is lower than the Predicted Sale Price, its the time to buy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitted By Lakshay Saxena"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6430e42904b6fa251db245d0198c9541e15eba0d9c0b96be23af22a9ba82a7ee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
